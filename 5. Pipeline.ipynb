{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "\n",
    "\n",
    "HOME = os.environ.get('HOME_PATH')\n",
    "VIDEO_PATH = os.path.join(HOME, \"pipeline\", \"1. videos\")\n",
    "FRAME_PATH = os.path.join(HOME, \"pipeline\", \"2. frames\")\n",
    "FRAME_PATH_UNIQUE = os.path.join(HOME, \"pipeline\", \"2. frames_unique\")\n",
    "FRAME_PATH_DROP = os.path.join(HOME, \"pipeline\", \"2. frames_duplicate_dropped\")\n",
    "FRAME_PATH_DISTINCT = os.path.join(HOME, \"pipeline\", \"2. frames_distinct\")\n",
    "FRAME_PATH_SIMILAR_DROP = os.path.join(HOME, \"pipeline\", \"2. frames_similar_drop\")\n",
    "WINDOW_PATH = os.path.join(HOME, \"pipeline\", \"3. windows\")\n",
    "WINDOW_PATH_UNIQUE = os.path.join(HOME, \"pipeline\", \"3. windows_unique\")\n",
    "WINDOW_PATH_DROP = os.path.join(HOME, \"pipeline\", \"3. windows_dropped\")\n",
    "PREDICTED_VEHICLE_PATH = os.path.join(HOME, \"pipeline\", \"4. predicted_vehicles\")\n",
    "PREDICTED_NO_VEHICLE_PATH = os.path.join(HOME, \"pipeline\", \"4. predicted_no_vehicles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing methods\n",
    "# Attempt to abstract to allow different impelementation (cv2, pillow, etc)\n",
    "# class ImageManager:\n",
    "#     def __init__(self, name):    \n",
    "#         self.name = name\n",
    " \n",
    "#     def getImage(self):    \n",
    "#         raise NotImplementedError(\"Subclass must implement abstract method\")\n",
    "        \n",
    "#     def resize(self, (width, height)):    \n",
    "#         raise NotImplementedError(\"Subclass must implement abstract method\")\n",
    "    \n",
    "#     def dhash(self):\n",
    "#         raise NotImplementedError(\"Subclass must implement abstract method\")\n",
    "    \n",
    "# class PillowImageManager(ImageManager):\n",
    "#     def resize(self, (width, height)): \n",
    "        \n",
    "# class Cv2ImageManager(ImageManager):\n",
    "#     def resize(self, (width, height)): \n",
    "    \n",
    "# def resizeImagePillow(image, width):\n",
    "#     height = round(width * image.size[1]/image.size[0])\n",
    "#     image.resize((width,height))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess \n",
    "import time\n",
    "\n",
    "FFMPEG_BINARY = \"C:\\\\Program Files (x86)\\\\ffmpeg\\\\ffmpeg-4.1.1-win64-static\\\\bin\\\\ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitFramesFromVideo(src, dest):\n",
    "    print(\"Getting frames from \", src)\n",
    "    \n",
    "    videoName = os.path.split(src)[1]\n",
    "    pathlib.Path(dest).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    command = [ FFMPEG_BINARY,\n",
    "                '-i', src,\n",
    "                '-r', '1/1',\n",
    "                dest + '/' + videoName + '.frame-%03d.png']\n",
    "    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=10**8)\n",
    "    proc.stderr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitFramesFromVideoJob(src, dest):\n",
    "          \n",
    "    print(\"Checking \", src)\n",
    "    \n",
    "    for filename in glob.iglob(os.path.join(src, '**/*.mp4'), recursive=True):   \n",
    "\n",
    "        filepath =  os.path.join(src, filename)\n",
    "        splitFramesFromVideo(filepath, dest)\n",
    "\n",
    "        # remove file once processed\n",
    "        os.remove(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhash(image, hash_size = 8):\n",
    "    # Grayscale and shrink the image.\n",
    "    \n",
    "    image = image.convert('LA')\n",
    "    image = image.resize((hash_size+1,hash_size))\n",
    "   \n",
    "    pixels = [pixel for pixel, alpha in image.getdata()]\n",
    "    pixels = np.array(pixels).reshape((8, 9))\n",
    "    \n",
    "    #pixels = image\n",
    "    # Compare adjacent pixels.\n",
    "    difference = []\n",
    "    for row in range(hash_size):\n",
    "        for col in range(hash_size):\n",
    "            pixel_left = pixels[row, col]\n",
    "            pixel_right = pixels[row, col + 1]\n",
    "            difference.append(pixel_left > pixel_right)\n",
    "    # Convert the binary array to a hexadecimal string.\n",
    "    decimal_value = 0\n",
    "    hex_string = []\n",
    "    \n",
    "    for index, value in enumerate(difference):\n",
    "        if value:\n",
    "            decimal_value += 2**(index % 8)\n",
    "        if (index % 8) == 7:\n",
    "            hex_string.append(hex(decimal_value)[2:].rjust(2, '0'))\n",
    "            decimal_value = 0\n",
    "    return ''.join(hex_string)\n",
    "\n",
    "\n",
    "def getImage(path):\n",
    "    image = cv2.imread(path)\n",
    "#     image = image.reshape((50, 100,3))\n",
    "#     image = Image.open(path)\n",
    "    return image\n",
    "\n",
    "def get_similar_score(img_a, img_b):\n",
    "    \n",
    "    SIMILAR_THRESHOLD = 10\n",
    "    \n",
    "    image_pixel_count = np.prod(img_a.shape)\n",
    "    image_chanel_pixel_count = image_pixel_count / 3\n",
    "    \n",
    "    if img_a.shape == img_b.shape :\n",
    "        \n",
    "        difference = cv2.subtract(img_a, img_b)\n",
    "        \n",
    "        # Treat small differences as zero\n",
    "        difference[difference < SIMILAR_THRESHOLD] = 0\n",
    "        b = difference[:,:,0] \n",
    "        g = difference[:,:,1] \n",
    "        r = difference[:,:,2] \n",
    "        \n",
    "        return np.sum(difference) / image_pixel_count\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"Images must be same size.\") \n",
    "\n",
    "# def get_sub_image(image):\n",
    "#     left = 480\n",
    "#     width = 1320\n",
    "#     top = 400\n",
    "#     height = 900\n",
    "\n",
    "#     return image[top:height,left:width,:] \n",
    "\n",
    "def get_mse(image_a, image_b):\n",
    "    sub_a = get_sub_image(image_a)\n",
    "    sub_b = get_sub_image(image_b)\n",
    "    err = np.sum((sub_a.astype(\"float\") - sub_b.astype(\"float\")) ** 2)\n",
    "    image_pixel_count = np.prod(sub_a.shape)\n",
    "    err /= float(image_pixel_count)\n",
    "    return err\n",
    "\n",
    "CHANGE_RATE = 0.065\n",
    "TUMBNAIL_SIZE = 30\n",
    "SUB_IMAGE_CENTER = (int(1920 / 2), int(1080 / 2) + 100)\n",
    "SUB_IMAGE_RADIUS = 250\n",
    "\n",
    "def get_sub_image(image):\n",
    "    left = SUB_IMAGE_CENTER[0] - SUB_IMAGE_RADIUS\n",
    "    width = left + SUB_IMAGE_RADIUS * 2\n",
    "    top = SUB_IMAGE_CENTER[1] - SUB_IMAGE_RADIUS\n",
    "    height = top + SUB_IMAGE_RADIUS * 2\n",
    "\n",
    "    return image[top:height,left:width,:] \n",
    "    \n",
    "def img_gray(image):    \n",
    "    return np.average(image, weights=[0.299, 0.587, 0.114], axis=2)\n",
    "\n",
    "def resize_image(image, height=TUMBNAIL_SIZE, width=TUMBNAIL_SIZE):\n",
    "    row_res = cv2.resize(image, (height, width), interpolation = cv2.INTER_AREA).flatten()\n",
    "    col_res = cv2.resize(image, (height, width), interpolation = cv2.INTER_AREA).flatten('F')\n",
    "    return row_res, col_res\n",
    "\n",
    "def intensity_diff(row_res, col_res):\n",
    "    # Checks if neighbor pixel is greater or less.\n",
    "    difference_row = np.diff(row_res)\n",
    "    difference_col = np.diff(col_res)\n",
    "    difference_row = difference_row > 0\n",
    "    difference_col = difference_col > 0\n",
    "    return np.vstack((difference_row, difference_col)).flatten()\n",
    "\n",
    "def image_hash(image, height = TUMBNAIL_SIZE, width = TUMBNAIL_SIZE):\n",
    "    sub_image = get_sub_image(image)\n",
    "    gray = img_gray(sub_image)    \n",
    "    row_res, col_res = resize_image(sub_image, height, width)\n",
    "    image_hash = intensity_diff(row_res, col_res)\n",
    "\n",
    "    return image_hash\n",
    "    \n",
    "def hamming_distance(image, image2):\n",
    "    score = scipy.spatial.distance.hamming(image, image2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_threshold(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.medianBlur(gray,5)\n",
    "    _,th2 = cv2.threshold(gray,124, 255,cv2.THRESH_BINARY)\n",
    "    th2 = cv2.cvtColor(th2, cv2.COLOR_GRAY2BGR)\n",
    "    return th2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image(image):\n",
    "    return binary_threshold(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicatesJob(src, dest, dropped_dest):\n",
    "    \n",
    "    pathlib.Path(dest).mkdir(parents=True, exist_ok=True) \n",
    "    pathlib.Path(dropped_dest).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    print(\"Check for duplicates in \", src)\n",
    "    # Add image hash to dictionary\n",
    "    imageDictionary = dict()\n",
    "\n",
    "    i = 0\n",
    "    for filename in glob.iglob(os.path.join(src, '**/*.png'), recursive=True):\n",
    "        \n",
    "        img = getImage(filename)\n",
    "        idx = dhash(img)\n",
    "        if idx not in imageDictionary:\n",
    "            imageDictionary[idx] = filename\n",
    "\n",
    "        else: \n",
    "            shutil.move(filename, dropped_dest)\n",
    "            i += 1\n",
    "#         imageDictionary[idx].append(filename)\n",
    "       \n",
    "    print(\"removed \" +str(i) + \" files\")\n",
    "    \n",
    "    # move remaining to dest\n",
    "    i = 0\n",
    "    for filename in glob.iglob(os.path.join(src, '**/*.png'), recursive=True):\n",
    "        shutil.move(filename, dest)\n",
    "        i += 1\n",
    "\n",
    "    print(\"kept \" +str(i) + \" files\")\n",
    "    \n",
    "\n",
    "def remove_similar_images_job_hamming(src, dest, dropped_dest):\n",
    "    print(\"Check for distinct images - hamming\", src)\n",
    "    \n",
    "\n",
    "\n",
    "    # Create folder if not already exists\n",
    "    pathlib.Path(dest).mkdir(parents=True, exist_ok=True) \n",
    "    pathlib.Path(dropped_dest).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    distinct_images = []\n",
    "    similar_images = []\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    \n",
    "    file_names = list(glob.iglob(os.path.join(src, '**/*.png'), recursive=True))\n",
    "    \n",
    "    # get first  \n",
    "    base_filename = file_names[0]\n",
    "    base_image = getImage(base_filename)\n",
    "    base_image = filter_image(base_image) # This is our filter image    \n",
    "    base_image_hash = image_hash(base_image)\n",
    "    distinct_images.append(base_filename)\n",
    "    \n",
    "    for file_name in file_names[1:]:\n",
    "        compare_image = getImage(file_name)    \n",
    "        compare_image = filter_image(compare_image)\n",
    "        compare_image_hash = image_hash(compare_image)\n",
    "        hd = hamming_distance(base_image_hash, compare_image_hash)  \n",
    "     \n",
    "        if(hd > CHANGE_RATE ):\n",
    "            base_filename = file_name\n",
    "            base_image_hash = compare_image_hash \n",
    "            distinct_images.append(file_name) \n",
    "        else:\n",
    "            similar_images.append(file_name)\n",
    "            \n",
    "   \n",
    "    #-------------\n",
    "    for file_name in distinct_images:\n",
    "        dest_filename = os.path.join(dest, os.path.basename(file_name))\n",
    "        shutil.move(file_name, dest_filename)\n",
    "        \n",
    "    for file_name in similar_images:\n",
    "        dest_filename = os.path.join(dropped_dest, os.path.basename(file_name))\n",
    "        shutil.move(file_name,dest_filename)\n",
    "        \n",
    "    print(\"Kept \" + str(len(distinct_images)) + \" images. Dropped \" + str(len(similar_images)))\n",
    "    \n",
    "    \n",
    "# def remove_similar_images_job_mse(src, dest, dropped_dest):\n",
    "    \n",
    "#     print(\"Check for distinct images \", src)\n",
    "    \n",
    "#     # Create folder if not already exists\n",
    "#     pathlib.Path(dest).mkdir(parents=True, exist_ok=True) \n",
    "#     pathlib.Path(dropped_dest).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "#     THRESHOLD_RATE = 1000\n",
    "    \n",
    "#     distinct_images = []\n",
    "#     similar_images = []\n",
    "    \n",
    "#     i = 0\n",
    "#     j = 0\n",
    "#     file_generator = glob.iglob(os.path.join(src, '**/*.png'), recursive=True)\n",
    "    \n",
    "#     # Treat first item as unique\n",
    "#     base_filename = next(file_generator)\n",
    "#     base_image = getImage(base_filename)\n",
    "#     distinct_images.append(base_filename)\n",
    "#     i += 1\n",
    "        \n",
    "#     for filename in file_generator:\n",
    "        \n",
    "#         compare_image = getImage(filename)\n",
    "        \n",
    "#         mse = get_mse(base_image, compare_image )\n",
    "#         print(mse, base_filename, \" with \", filename)\n",
    "        \n",
    "#         if(mse < THRESHOLD_RATE):\n",
    "#             print(\"Add to similar\")\n",
    "#             similar_images.append(filename)\n",
    "#         else:\n",
    "#             print(\"Add to distinct\")\n",
    "#             base_filename = filename\n",
    "#             base_image = getImage(base_filename)\n",
    "#             distinct_images.append(filename)\n",
    "            \n",
    "#     for filename in distinct_images:\n",
    "#         shutil.move(filename,dest)\n",
    "        \n",
    "#     for filename in similar_images:\n",
    "#         shutil.move(filename,dropped_dest)\n",
    "        \n",
    "#     print(\"Kept \" + str(len(distinct_images)) + \" images. Dropped \" + str(len(similar_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hamming_score(src):\n",
    "    \n",
    "    THRESHOLD_RATE = 1000\n",
    "    \n",
    "    distinct_images = []\n",
    "    similar_images = []\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    file_names = list(glob.iglob(os.path.join(src, '**/*.png'), recursive=True))\n",
    "    \n",
    "    # Treat first item as unique\n",
    "    base_filename = file_names[0]\n",
    "    base_image = getImage(base_filename)\n",
    "    base_image = filter_image(base_image) # This is our filter image    \n",
    "    base_image_hash = image_hash(base_image)\n",
    "    distinct_images.append(base_filename)\n",
    "    i += 1\n",
    "        \n",
    "    for file_name in file_names[1:]:\n",
    "        compare_image = getImage(file_name)    \n",
    "        compare_image = filter_image(compare_image)\n",
    "        compare_image_hash = image_hash(compare_image)\n",
    "        hd = hamming_distance(base_image_hash, compare_image_hash)        \n",
    "        print(hd, base_filename.split('\\\\')[-1], \" with \", file_name.split('\\\\')[-1])\n",
    "\n",
    "test_hamming_score(FRAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import MiniBatchKMeans\n",
    "# def test_color_quantization(src):\n",
    "    \n",
    "#     N_CLUSTERS = 3\n",
    "    \n",
    "#     file_names = list(glob.iglob(os.path.join(src, '**/*.png'), recursive=True))\n",
    "    \n",
    "#     # Treat first item as unique\n",
    "#     base_filename = file_names[0]\n",
    "#     image = getImage(base_filename)\n",
    "#     plt.imshow(image)\n",
    "    \n",
    "#     (h, w) = image.shape[:2]\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "#     image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "#     clt = MiniBatchKMeans(N_CLUSTERS)\n",
    "#     labels = clt.fit_predict(image)\n",
    "#     quant = clt.cluster_centers_.astype(\"uint8\")[labels]\n",
    "    \n",
    "#     quant = quant.reshape((h, w, 3))\n",
    "#     image = image.reshape((h, w, 3))\n",
    "    \n",
    "#     quant = cv2.cvtColor(quant, cv2.COLOR_LAB2BGR)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "#     print(\"Hello\")\n",
    "    \n",
    "#     cv2.imshow(\"image\", np.hstack([image, quant]))\n",
    "    \n",
    "#     plt.imshow(quant)\n",
    "    \n",
    "# test_color_quantization(FRAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# def test_median_filter(src):\n",
    "#     file_names = list(glob.iglob(os.path.join(src, '**/*.png'), recursive=True))\n",
    "    \n",
    "#     # Treat first item as unique\n",
    "#     base_filename = file_names[0]\n",
    "#     image = getImage(base_filename)\n",
    "# #     plt.imshow(image)\n",
    "    \n",
    "    \n",
    "#     # create the argument parser and parse the arguments\n",
    "# #     ap = argparse.ArgumentParser()\n",
    "# #     ap.add_argument('-i', '--image', required = True, help = base_filename)\n",
    "# #     args = vars(ap.parse_args())\n",
    "    \n",
    "#     processed_image = cv2.medianBlur(image, 9)\n",
    "#     plt.imshow(processed_image)\n",
    "\n",
    "# test_median_filter(FRAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "          \n",
    "# def test_errosion(src):\n",
    "    \n",
    "    \n",
    "          \n",
    "#     file_names = list(glob.iglob(os.path.join(src, '**/*.png'), recursive=True))\n",
    "    \n",
    "#     # Treat first item as unique\n",
    "#     base_filename = file_names[0]\n",
    "#     image = getImage(base_filename)\n",
    "          \n",
    "#     kernel = np.ones((9,9),np.uint8)\n",
    "#     erosion = cv2.erode(image,kernel,iterations = 1)    \n",
    "    \n",
    "#     plt.imshow(erosion)\n",
    "    \n",
    "# test_errosion(FRAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import shuffle\n",
    "\n",
    "\n",
    "# HOME = os.environ.get('HOME_PATH')\n",
    "# LOCAL_FRAME_PATH = os.path.join(HOME, \"local\", \"pipeline_2_frames\")\n",
    "# LOCAL_FRAME_PATH_DISTINCT = os.path.join(HOME, \"pipeline\", \"2_frames_distinct\")\n",
    "# LOCAL_FRAME_PATH_SIMILAR_DROP = os.path.join(HOME, \"pipeline\", \"2_frames_similar_drop\")\n",
    "# CONTAINER_FRAMES = 'pipeline-2-frames'\n",
    "# CONTAINER_FRAMES_DISTINCT = 'pipeline-2-frames-distinct'\n",
    "# CONTAINER_SIMILAR_DROP = 'pipeline-2-frames-similar-drop'\n",
    "\n",
    "\n",
    "# def doit():\n",
    "    \n",
    "#     # Create Local folders\n",
    "#     pathlib.Path(LOCAL_FRAME_PATH).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     blobs = []\n",
    "    \n",
    "#     file_generator = glob.iglob(os.path.join(FRAME_PATH, '**/*.png'), recursive=True)\n",
    "#     for filename in file_generator:\n",
    "#         blobs.append(filename.split('\\\\')[-1])\n",
    "        \n",
    "#     shuffle(blobs)\n",
    "#     print(blobs[:10] )\n",
    "    \n",
    "#     blobs.sort()\n",
    "#     print(blobs[:10])\n",
    "    \n",
    "#     # Get Local file\n",
    "#     for blob in blobs:\n",
    "#         dest_filename = os.path.join(VIDEO_PATH, blob.name)\n",
    "#         blob_service.get_blob_to_path(\n",
    "#             CONTAINER_VIDEOS,\n",
    "#             blob.name,\n",
    "#             dest_filename\n",
    "#             )\n",
    "    \n",
    "# doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid(image, scale=1.5, minSize=(100, 50)):\n",
    "    # yield the original image\n",
    "    pixels = np.array(image)\n",
    "    \n",
    "    yield pixels\n",
    " \n",
    "    # keep looping over the pyramid\n",
    "    while True:\n",
    "              \n",
    "        # compute the new dimensions of the image and resize it\n",
    "        w = int(pixels.shape[1] / scale)\n",
    "        image = imutils.resize(pixels, width=w)\n",
    " \n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    " \n",
    "        # yield the next image in the pyramid\n",
    "        yield pixels\n",
    "        \n",
    "def buildImageName(src, x, y, scale=1):\n",
    "    parts =  src.split('\\\\')\n",
    "    videoFile = parts[-1]\n",
    "    videoFileName = videoFile.split('.')[0]\n",
    "    \n",
    "    frameNo = parts[-1].split('.')[-2].split('-')[1]\n",
    "    \n",
    "    return videoFileName + '-' + frameNo + '-' + str(scale) + '-' + str(x) + '-' + str(y) + '.jpg'\n",
    "        \n",
    "def sliding_window(pixels, stepSize=20, windowSize=(100,50)):\n",
    "    \n",
    "    winW = windowSize[0]\n",
    "    winH = windowSize[1]\n",
    "    \n",
    "    # slide a window across the image\n",
    "    for y in range(0, pixels.shape[0], stepSize):\n",
    "        for x in range(0, pixels.shape[1], stepSize):\n",
    "            \n",
    "            window = pixels[y:y+windowSize[1],x:x+windowSize[0]]\n",
    "            \n",
    "            # if the window does not meet our desired window size, ignore it\n",
    "            if window.shape[0] != winH or window.shape[1] != winW:\n",
    "                continue\n",
    "                \n",
    "            # yield the current window\n",
    "            yield (x, y, window)\n",
    "        \n",
    "def splitWindowsFromFramesJob(src, dest, scale=1):\n",
    "    \n",
    "    paths = []\n",
    "    \n",
    "    print(\"Spltting Windows for \", src)\n",
    "    \n",
    "    # create dest if it doesnt' exist\n",
    "    pathlib.Path(dest).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    i = 0\n",
    "    for filename in glob.iglob(os.path.join(src, '**/*.png'), recursive=True):    \n",
    "        \n",
    "        print(\"Spltting Windows for \", filename)\n",
    "                \n",
    "        image = getImage(filename)\n",
    "        \n",
    "        for (i, resized) in enumerate(pyramid(image, scale=scale)):\n",
    "\n",
    "            for x, y, window in sliding_window(resized):    \n",
    "\n",
    "                destImageName = buildImageName(filename, x, y, scale=i+1)   \n",
    "                \n",
    "                w=Image.fromarray(window,mode='RGB')\n",
    "                fullpath =os.path.join(dest, destImageName) \n",
    "                w.save(fullpath)\n",
    "                paths.append(fullpath)\n",
    "                \n",
    "        if i > 2:\n",
    "            break\n",
    "    \n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient  \n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "PREDICTION_ENDPOINT = os.environ.get(\"PREDICTION_ENDPOINT\") \n",
    "PREDICTION_KEY =  os.environ.get(\"PREDICTION_KEY\") \n",
    "PREDICITON_RESOURCE_ID =  os.environ.get(\"PREDICITON_RESOURCE_ID\") \n",
    "\n",
    "PROJECT_ID =  os.environ.get(\"PROJECT_ID\") \n",
    "PUBLISH_ITERATION_NAME =  os.environ.get(\"PUBLISH_ITERATION_NAME\") \n",
    "\n",
    "\n",
    "url =  '{Endpoint}/{projectId}/detect/iterations/{publishedName}/image'\n",
    "path_format_arguments = {\n",
    "    'Endpoint': PREDICTION_ENDPOINT,\n",
    "    'projectId': PROJECT_ID,\n",
    "    'publishedName': PUBLISH_ITERATION_NAME\n",
    "}\n",
    "PREDICTION_URL = url.format(**path_format_arguments)\n",
    "\n",
    "HEADER_PARAMETERS = {\n",
    "    'accept' : 'application/json',\n",
    "    'content-type' : 'application/octet-stream',\n",
    "    'Prediction-Key' : PREDICTION_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score_prediction(prediction):\n",
    "    bounding_box =  prediction['boundingBox']\n",
    "    pre = {\n",
    "        'probability': prediction['probability'],\n",
    "        'tagName' : prediction['tagName'],\n",
    "        'bb_left' : bounding_box['left'],\n",
    "        'bb_top' : bounding_box['top'],\n",
    "        'bb_width' : bounding_box['width'],\n",
    "        'bb_height' : bounding_box['height']                        \n",
    "    }\n",
    "    return pre\n",
    "    print(\"{tagName} {probability} - bbox.left = {bb_left}, bbox.top = {bb_top}, bbox.width = {bb_width}, bbox.height = {bb_height}\".format(**pre))\n",
    "    \n",
    "def score_image(src):\n",
    "    \n",
    "    with open(src, mode=\"rb\") as image:\n",
    "    \n",
    "        response  = requests.post(url=PREDICTION_URL,\n",
    "                            data=image,\n",
    "                            headers=HEADER_PARAMETERS)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            results = json.loads(response.content.decode('utf-8'))\n",
    "\n",
    "            return results['predictions'] \n",
    "            \n",
    "        else:\n",
    "            print(response)\n",
    "            raise Exception('Failed to get score.', response.status_code)\n",
    "\n",
    "def get_top_prediction(predictions):\n",
    "    \n",
    "    top_score = 0\n",
    "    top_prediction = {}\n",
    "    \n",
    "    for prediction in predictions:\n",
    "                \n",
    "        if(prediction['probability'] > top_score ):\n",
    "            top_score = prediction['probability'] \n",
    "            top_prediction = prediction\n",
    "            \n",
    "    \n",
    "    return top_prediction\n",
    "        \n",
    "def save_prediction(filename, predictions, dest):\n",
    "    meta_filename = filename + '.annotation.json'\n",
    "    \n",
    "    full_path = os.path.join(dest, meta_filename)\n",
    "    \n",
    "    with open(full_path, 'w') as outfile:\n",
    "        json.dump(predictions, outfile)\n",
    "\n",
    "\n",
    "def test_score_image_save(filename, dest):\n",
    "    predictions = score_image(filename)\n",
    "    save_prediction(filename, predictions, dest)\n",
    "    \n",
    "def score_image_from_frames_job(src, predicted_vechicle_dest, predicted_no_vechicle_dest):\n",
    "    \n",
    "    score_threshold = 0.8\n",
    "    i = 0\n",
    "    paths = []\n",
    "    \n",
    "    \n",
    "    print(\"Scoring images in \", src)\n",
    "    \n",
    "    # create dest if it doesnt' exist\n",
    "    pathlib.Path(predicted_vechicle_dest).mkdir(parents=True, exist_ok=True) \n",
    "    pathlib.Path(predicted_no_vechicle_dest).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    for filename in glob.iglob(os.path.join(src, '**/*.png'), recursive=True):  \n",
    "        predictions = score_image(filename)\n",
    "        save_prediction(filename, predictions, src)\n",
    "        prediction = get_top_prediction(predictions)\n",
    "        \n",
    "        if(prediction):\n",
    "            if(prediction['probability'] > score_threshold ):\n",
    "               \n",
    "                dst_filename = os.path.join(predicted_vechicle_dest, os.path.basename(filename))\n",
    "                shutil.move(filename, dst_filename)\n",
    "                dst_filename_annon = os.path.join(predicted_vechicle_dest, os.path.basename(filename + '.annotation.json'))\n",
    "                shutil.move(filename + '.annotation.json', dst_filename_annon)\n",
    "                \n",
    "                i += 1\n",
    "            else :                \n",
    "                dst_filename = os.path.join(predicted_no_vechicle_dest, os.path.basename(filename))\n",
    "                shutil.move(filename, dst_filename)\n",
    "                dst_filename_annon = os.path.join(predicted_no_vechicle_dest, os.path.basename(filename + '.annotation.json'))\n",
    "                shutil.move(filename + '.annotation.json', dst_filename_annon)\n",
    "                j += 1\n",
    "        else:\n",
    "            dst_filename = os.path.join(predicted_no_vechicle_dest, os.path.basename(filename))\n",
    "            shutil.move(filename, dst_filename)\n",
    "            dst_filename_annon = os.path.join(predicted_no_vechicle_dest, os.path.basename(filename + '.annotation.json'))\n",
    "            shutil.move(filename + '.annotation.json', dst_filename_annon)\n",
    "            j += 1\n",
    "                \n",
    "                \n",
    "    print(\"Labeled \" +str(i) + \" files\")\n",
    "    print(\"Dropped \" +str(j) + \" files\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image_path = os.path.join(HOME_PATH, 'frames\\\\20180326_071212_NF.mp4\\\\frame-003.png')\n",
    "# test_image_save_path = os.path.join(HOME_PATH, 'frames\\\\20180326_071212_NF.mp4\\\\')\n",
    "# test_score_image_save(test_image_path, test_image_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_predictions(image, predictions):\n",
    "    \n",
    "    im = np.array(Image.open(test_image_path), dtype=np.uint8)\n",
    "\n",
    "    plt.figure(num=None, figsize=(15 ,15), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.imshow( im )\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        \n",
    "        bbox_left = 1920 * prediction['boundingBox']['left']\n",
    "        bbox_top = 1080 * prediction['boundingBox']['top']\n",
    "        bbox_width = 1920 * prediction['boundingBox']['width']\n",
    "        bbox_height = 1080 * prediction['boundingBox']['height']\n",
    "       \n",
    "        ax = plt.gca()\n",
    "        rect = patches.Rectangle((bbox_left,bbox_top),bbox_width,bbox_height,linewidth=1,edgecolor='r',facecolor='none')\n",
    "        ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = score_image(test_image_path)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preditions = json.loads('[{\"probability\": 0.4296951, \"tagId\": \"991ba077-b9b4-4538-a6e5-8eab7e5daf1e\", \"tagName\": \"license\", \"boundingBox\": {\"left\": 0.6178843, \"top\": 0.6097945, \"width\": 0.06222242, \"height\": 0.0467823148}}, {\"probability\": 0.0109245218, \"tagId\": \"991ba077-b9b4-4538-a6e5-8eab7e5daf1e\", \"tagName\": \"license\", \"boundingBox\": {\"left\": 0.1954996, \"top\": 0.946872056, \"width\": 0.08564204, \"height\": 0.05311793}}, {\"probability\": 0.242702723, \"tagId\": \"71adc55b-fa59-4cc0-8d43-863193b45ecc\", \"tagName\": \"vehicle\", \"boundingBox\": {\"left\": 0.770755768, \"top\": 0.5213496, \"width\": 0.08635795, \"height\": 0.110935867}}, {\"probability\": 0.07627616, \"tagId\": \"71adc55b-fa59-4cc0-8d43-863193b45ecc\", \"tagName\": \"vehicle\", \"boundingBox\": {\"left\": 0.847536147, \"top\": 0.5082251, \"width\": 0.0618121028, \"height\": 0.103719473}}, {\"probability\": 0.9402402, \"tagId\": \"71adc55b-fa59-4cc0-8d43-863193b45ecc\", \"tagName\": \"vehicle\", \"boundingBox\": {\"left\": 0.5640773, \"top\": 0.461323023, \"width\": 0.211154938, \"height\": 0.302533269}}]')\n",
    "show_predictions(test_image_path, preditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitFramesFromVideoJob(VIDEO_PATH, FRAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removeDuplicatesJob(FRAME_PATH, FRAME_PATH_UNIQUE, FRAME_PATH_DROP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_similar_images_job_hamming(FRAME_PATH, FRAME_PATH_DISTINCT, FRAME_PATH_SIMILAR_DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.move(os.path.join(HOME_PATH, 'pipeline\\\\2. frames\\\\20190330_165033_EF.mp4.frame-014.png'), os.path.join(HOME_PATH, 'pipeline\\\\2. frames_distinct\\\\20190330_165033_EF.mp4.frame-014.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_image_from_frames_job(FRAME_PATH_DISTINCT, PREDICTED_VEHICLE_PATH, PREDICTED_NO_VEHICLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All at once\n",
    "# splitFramesFromVideoJob(VIDEO_PATH, FRAME_PATH)\n",
    "# removeDuplicatesJob(FRAME_PATH, FRAME_PATH_UNIQUE, FRAME_PATH_DROP )\n",
    "# score_image_from_frames_job(FRAME_PATH_UNIQUE, PREDICTED_VEHICLE_PATH, PREDICTED_NO_VEHICLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = splitWindowsFromFramesJob(FRAME_PATH_UNIQUE, WINDOW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_image = getImage(os.path.join(HOME_PATH, 'pipeline\\\\2. frames_unique\\\\20180326_071212_NF.mp4-frame-001.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_image.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_pixels = np.array(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_pixels.shape)\n",
    "# x_width = 64\n",
    "\n",
    "# x_height = round(x_width * x_image.size[1]/x_image.size[0])\n",
    "\n",
    "# print(x_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_src = os.path.join(HOME_PATH, 'pipeline\\\\2. frames_unique\\\\20180326_071212_NF.mp4.frame-001.png')\n",
    "# x_parts =  x_src.split('\\\\')\n",
    "# print(x_parts)\n",
    "\n",
    "# x_videoFile = x_parts[-1].split(\".\")\n",
    "# print(x_videoFile)\n",
    "\n",
    "# x_frameNo = x_parts[-1].split(\".\")[-2]\n",
    "# print(x_frameNo)\n",
    "\n",
    "# # want the format 20180326_071212_NF-006-1-0-0\n",
    "\n",
    "# buildImageName(x_src,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
